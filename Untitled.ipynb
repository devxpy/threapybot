{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f09c669-9a7d-443b-88df-359eebc031a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGING_FACE_HUB_TOKEN'] = xxxx\n",
    "os.environ['QDRANT_API_KEY'] = xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562b8b3a-1b29-421e-acd5-20404631fe2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/therapybot/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, GenerationConfig\n",
    "\n",
    "from trl import SFTTrainer,  DataCollatorForCompletionOnlyLM\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f16bf9-dd98-4e4a-86d0-7ff87295e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd9d778-7c16-468d-bcb5-f5bd512ea188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and parse arguments.\n",
    "class ScriptArgs:\n",
    "    \"\"\"\n",
    "    The name of the Casual LM model we wish to fine with SFTTrainer\n",
    "    \"\"\"\n",
    "\n",
    "    model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "    # dataset_name = \"timdettmers/openassistant-guanaco\" # the dataset name\n",
    "    dataset_text_field = \"text\" # the text field of the dataset\"\n",
    "    log_with = \"wandb\"\n",
    "    learning_rate = 1.41e-5\n",
    "    batch_size = 16 # 64\n",
    "    seq_length = 512\n",
    "    gradient_accumulation_steps = 4 # 16\n",
    "    load_in_8bit = False\n",
    "    load_in_4bit = True\n",
    "    use_peft = True\n",
    "    trust_remote_code = True\n",
    "    output_dir = \"output\"  # the output directory\"\n",
    "    peft_lora_r = 64  # the r parameter of the LoRA adapters\"\n",
    "    peft_lora_alpha = 16  # the alpha parameter of the LoRA adapters\"\n",
    "    logging_steps = 1  # the number of logging steps\"\n",
    "    use_auth_token = True  # Use HF auth token to access the model\"\n",
    "    num_train_epochs = 3  # the number of training epochs\"\n",
    "    max_steps = -1  # the number of training steps\"\n",
    "    save_steps = 100  # Number of updates steps before two checkpoint saves\n",
    "    save_total_limit = 10  # metadata={\"help\": \"Limits total number of checkpoints.\"})\n",
    "    push_to_hub = False  # metadata={\"help\": \"Push the model to HF Hub\"})\n",
    "    hub_model_id = None  # metadata={\"help\": \"The name of the model on HF Hub\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814af78e-51bd-451b-ab97-bbc6d6997330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a109f3de-e480-440e-87d5-ffc3560618dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/therapybot/venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:460: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.95s/it]\n",
      "/home/dev/therapybot/venv/lib/python3.8/site-packages/transformers/utils/hub.py:373: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the model\n",
    "if ScriptArgs.load_in_8bit and ScriptArgs.load_in_4bit:\n",
    "    raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\")\n",
    "elif ScriptArgs.load_in_8bit or ScriptArgs.load_in_4bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=ScriptArgs.load_in_8bit, load_in_4bit=ScriptArgs.load_in_4bit\n",
    "    )\n",
    "    # This means: fit the entire model on the GPU:0\n",
    "    device_map = {\"\": 0}\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    device_map = None\n",
    "    quantization_config = None\n",
    "    torch_dtype = None\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    ScriptArgs.model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=ScriptArgs.trust_remote_code,\n",
    "    torch_dtype=torch_dtype,\n",
    "    use_auth_token=ScriptArgs.use_auth_token,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ScriptArgs.model_name)\n",
    "# tokenizer.all_special_tokens_extended\n",
    "\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False # Gradient checkpointing is used by default but not compatible with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365095e3-2ffe-47cb-b496-6c752da6931e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e37389f-6f7e-48ea-9316-817616db8ebf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# df = pd.read_csv(\"./[Therapy] Web Content - out.csv\")\n",
    "df = pd.read_csv(\"./f (1).csv\")\n",
    "# df.iloc[:500]\n",
    "# df = df.dropna()\n",
    "# convos = df.transcript.tolist()\n",
    "\n",
    "convo_re = re.compile(r\"^(.{0,50})\\:\", flags=re.M,)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for entry in df.itertuples():\n",
    "    convo = json.loads(entry.conversation)\n",
    "    if not convo:\n",
    "        continue\n",
    "    \n",
    "    if convo[0][\"role\"] == \"assistant\":\n",
    "        convo.insert(0, {'role': 'user', 'message': \"Hello\"})        \n",
    "\n",
    "    # for i in range(0, len(convo), 10):\n",
    "    text = tokenizer.bos_token\n",
    "    system_prompt = entry.title or \"\"\n",
    "    text += f\"<<SYS>>{system_prompt}<</SYS>>\"\n",
    "    \n",
    "    for idx, msg in enumerate(convo): # enumerate(convo[i: i + 10]):\n",
    "        line = \"### \" + msg['role'].lower() + \": \" + msg['message'].replace(\"\\n\", \" \").strip()\n",
    "\n",
    "        if idx % 2 == 1:\n",
    "            line += tokenizer.eos_token\n",
    "        else:\n",
    "            if idx < 1:\n",
    "                line = \" [INST] \" + line + \" [/INST] \"\n",
    "            else:\n",
    "                line = \"\\n\" + tokenizer.bos_token + \"[INST] \" + line + \" [/INST] \"\n",
    "        \n",
    "        text += line\n",
    "        \n",
    "    rows.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6d5c1e-bfa3-411f-a8ef-ce9fea2e656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(\n",
    "    pd.DataFrame(rows, columns=[\"text\"])\n",
    ")\n",
    "dataset = dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccd6c3bc-647d-4623-b3da-570d9f93a5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 33.87ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  8.96it/s]\n",
      "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 75.09ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  7.02it/s]\n",
      "Downloading metadata: 100%|██████████| 559/559 [00:00<00:00, 3.27MB/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.push_to_hub(\"devxpy/therapychat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "598594c1-2b55-4d1a-b1e4-50b89008b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_special_tokens({'pad_token': '[INST]'})\n",
    "# tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "# tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "# tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8268fa8-6f36-48fd-9c00-29a11af7c1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/therapybot/venv/lib/python3.8/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "Map: 100%|██████████| 516/516 [00:00<00:00, 896.22 examples/s]\n",
      "Map: 100%|██████████| 222/222 [00:00<00:00, 783.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    instruction_template=\"### user:\",\n",
    "    response_template=\"### assistant:\", \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Step 3: Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=ScriptArgs.output_dir,\n",
    "    per_device_train_batch_size=ScriptArgs.batch_size,\n",
    "    gradient_accumulation_steps=ScriptArgs.gradient_accumulation_steps,\n",
    "    learning_rate=ScriptArgs.learning_rate,\n",
    "    logging_steps=ScriptArgs.logging_steps,\n",
    "    num_train_epochs=ScriptArgs.num_train_epochs,\n",
    "    max_steps=ScriptArgs.max_steps,\n",
    "    report_to=ScriptArgs.log_with,\n",
    "    save_steps=ScriptArgs.save_steps,\n",
    "    save_total_limit=ScriptArgs.save_total_limit,\n",
    "    push_to_hub=ScriptArgs.push_to_hub,\n",
    "    hub_model_id=ScriptArgs.hub_model_id,\n",
    ")\n",
    "\n",
    "# Step 4: Define the LoraConfig\n",
    "if ScriptArgs.use_peft:\n",
    "    peft_config = LoraConfig(\n",
    "        r=ScriptArgs.peft_lora_r,\n",
    "        lora_alpha=ScriptArgs.peft_lora_alpha,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "else:\n",
    "    peft_config = None\n",
    "\n",
    "\n",
    "# Step 5: Define the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    max_seq_length=ScriptArgs.seq_length,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    dataset_text_field=ScriptArgs.dataset_text_field,\n",
    "    peft_config=peft_config,\n",
    "    # data_collator=collator,\n",
    "    # tokenizer=tokenizer,\n",
    "    # special_tokens=[\"[MASK]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e79947e-8032-4148-8f08-0740c8fade51",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "164cc896-d29b-4045-9012-5fed26ea7881",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mdevxpy\u001B[0m (\u001B[33mgooeyai\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dev/therapybot/wandb/run-20230902_155230-gbpo77ax</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gooeyai/huggingface/runs/gbpo77ax' target=\"_blank\">icy-night-20</a></strong> to <a href='https://wandb.ai/gooeyai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gooeyai/huggingface' target=\"_blank\">https://wandb.ai/gooeyai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gooeyai/huggingface/runs/gbpo77ax' target=\"_blank\">https://wandb.ai/gooeyai/huggingface/runs/gbpo77ax</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 28:35, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.982600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.013500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.966800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.966100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.943600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.914200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.955700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.947800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.957400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.930500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.880400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.940100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.943200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.900600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24, training_loss=2.9553772608439126, metrics={'train_runtime': 1794.7336, 'train_samples_per_second': 0.863, 'train_steps_per_second': 0.013, 'total_flos': 1.580598615343104e+16, 'train_loss': 2.9553772608439126, 'epoch': 2.91})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a70c3624-602a-4bef-a63f-75cbdd755e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Save the model\n",
    "trainer.save_model(ScriptArgs.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb52d63-a990-4162-a5cc-27d98e551caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dc211c-e999-4679-8d38-9ac09fdd54f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da7c2bf0-2bde-4896-bf95-11efa119ba90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "adapter_config.json  special_tokens_map.json  tokenizer.model\n",
      "adapter_model.bin    tokenizer_config.json    training_args.bin\n",
      "README.md\t     tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!ls output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f107fba-f363-4e9b-beab-718c3c50eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "ftmodel = PeftModel.from_pretrained(model, \"./output/\", torch_dtype=torch.float16,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2044ab95-d037-4a28-bad5-f4d0b24b17bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.bin: 100%|██████████| 134M/134M [00:03<00:00, 44.0MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/devxpy/therapybot/commit/6f5d5729ba425d05b8d8fecca05fbc520d7d1da8', commit_message='Upload model', commit_description='', oid='6f5d5729ba425d05b8d8fecca05fbc520d7d1da8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftmodel.push_to_hub(\"devxpy/therapybot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "df2cd601-cac1-4344-97a7-1b690e9fa091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>>\n",
      "Transcript of patient with Dr. Brown\n",
      "<</SYS>>\n",
      "[INST] ### user: Hi! [/INST] ### assistant: Hello! How are you?\n",
      "[INST] ### user: I am feeling tired [/INST] ### assistant: Why?\n",
      "[INST] ### user: I have worked too much [/INST] ### assistant: What do you do?\n",
      "[INST] ### user: I do programming [/INST] ### assistant: What kind of programming\n",
      "[INST] ### user: I program in python to create ai and its not working, suggest me some medicines [/INST]\n"
     ]
    }
   ],
   "source": [
    "#print(correct_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42b01777-a368-4629-8f48-9dcb56a57b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://embed.spotify.com/?uri=spotify:playlist:3Fv3vd2eqb9rVvIJcJxEoU',\n",
       " 'https://embed.spotify.com/?uri=spotify:playlist:2uM2IvAqc6HZmEiRjJBF6Q',\n",
       " 'https://embed.spotify.com/?uri=spotify:playlist:54j0DPxOen62HcPCc2Pmoe',\n",
       " 'https://embed.spotify.com/?uri=spotify:playlist:1uN3iCLmAXg6rmS86SJM7g',\n",
       " 'https://embed.spotify.com/?uri=spotify:playlist:703bHAXaUiMA90jlQyemQb',\n",
       " 'https://embed.spotify.com/?uri=spotify:playlist:4uLg82FvdD0blplTBTmR59',\n",
       " 'https://embed.spotify.com/?uri=spotify:playlist:4SJlsd4gR9A8DTzRoMrFPl',\n",
       " 'https://embed.spotify.com/?uri=spotify:playlist:4pJ4ZEzj93WnpU4CUgVxH1',\n",
       " 'https://embed.spotify.com/?uri=spotify:playlist:0QrUMIfELgl7gi1zYcCgq4']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import requests_html\n",
    "import torch\n",
    "from qdrant_client import models, QdrantClient\n",
    "from qdrant_client.models import VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "COLLECTION_NAME = \"spotify-genres\"\n",
    "\n",
    "encoder = SentenceTransformer(\n",
    "    \"thenlper/gte-large\", device=\"mps\" if torch.has_mps else \"cuda\"\n",
    ")  # thenlper/gte-base\n",
    "\n",
    "\n",
    "def get_playlists(q, limit=9):\n",
    "    # Create a client object for Qdrant.\n",
    "    qdrant = QdrantClient(\n",
    "        \"https://303757f4-9127-4df0-a9e8-9669f997f742.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
    "        api_key=os.environ['QDRANT_API_KEY'],\n",
    "    )\n",
    "    hits = qdrant.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query_vector=encoder.encode(q).tolist(),\n",
    "        limit=limit,\n",
    "    )\n",
    "    return [x.payload[\"playlist_url\"] for x in hits]\n",
    "\n",
    "get_playlists(\"sad and angry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6e4794a3-6422-4037-982b-7d0285a73e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  I am very bored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### assistant: Bored? You can always watch the news. \n",
      "Playlists:  https://embed.spotify.com/?uri=spotify:playlist:5PI1ISPDdvY2InHQ4ltty1, https://embed.spotify.com/?uri=spotify:playlist:6zeeSXGL4cJQ8pM7tsv42f, https://embed.spotify.com/?uri=spotify:playlist:1pkHHdvmo3uvqjXXcumTja\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  I feel really happy and excited now!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### assistant: You can always go to the cinema.\n",
      "\n",
      "Playlists:  https://embed.spotify.com/?uri=spotify:playlist:17TwTPITOWP0Ep9joASCrm, https://embed.spotify.com/?uri=spotify:playlist:4aId92f7OaneHjOx0emyjz, https://embed.spotify.com/?uri=spotify:playlist:0nrHQMazFvwjMpg5esflnT\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  The cinema makes me really sad and gloomy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### assistant: So do you want to go home?\n",
      "\n",
      "Playlists:  https://embed.spotify.com/?uri=spotify:playlist:6mDBTlpxmqfCQXQalGJOxe, https://embed.spotify.com/?uri=spotify:playlist:1l4otzwhS8LnLsWKducSxa, https://embed.spotify.com/?uri=spotify:playlist:7oNPNXqyCxty9nHMltU1pp\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Yeah, i just wanna rest in my bed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### assistant: Ok, then i am gonna leave.\n",
      "\n",
      "Playlists:  https://embed.spotify.com/?uri=spotify:playlist:2pSeH1YQCSICs2knjs7e5o, https://embed.spotify.com/?uri=spotify:playlist:2vLez030U00PBsuE2ug926, https://embed.spotify.com/?uri=spotify:playlist:2owRoUVlHqTdJF3FmF5uwZ\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Okie bye! See you later\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### assistant: Bye, have a nice evening!\n",
      "\n",
      "Playlists:  https://embed.spotify.com/?uri=spotify:playlist:5PI1ISPDdvY2InHQ4ltty1, https://embed.spotify.com/?uri=spotify:playlist:3OJiIetHH3ef9mwP8e3gH5, https://embed.spotify.com/?uri=spotify:playlist:1pkHHdvmo3uvqjXXcumTja\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "<<SYS>>\n",
      "Transcript of patient with Dr. Brown\n",
      "<</SYS>>\n",
      "[INST] ### user: Hi! [/INST] ### assistant: Hello! How are you?\n",
      "[INST] ### user: I am very bored [/INST]### assistant: Bored? You can always watch the news.\n",
      "[INST] ### user: I feel really happy and excited now! [/INST]### assistant: You can always go to the cinema.\n",
      "[INST] ### user: The cinema makes me really sad and gloomy [/INST]### assistant: So do you want to go home?\n",
      "[INST] ### user: Yeah, i just wanna rest in my bed [/INST]### assistant: Ok, then i am gonna leave.\n",
      "[INST] ### user: Okie bye! See you later [/INST]### assistant: Bye, have a nice evening!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "<<SYS>>\\nTranscript of patient with Dr. Brown\\n<</SYS>>\n",
    "[INST] ### user: Hi! [/INST] ### assistant: Hello! How are you?\n",
    "\"\"\".strip()\n",
    "\n",
    "while True:\n",
    "    msg = input(\"User: \").strip()\n",
    "    if not msg:\n",
    "        break\n",
    "    \n",
    "    prompt += f\"\\n[INST] ### user: {msg} [/INST]\"\n",
    "    # print(prompt)\n",
    "    \n",
    "    inputs = tokenizer.encode(prompt.strip(), return_tensors=\"pt\")\n",
    "            \n",
    "    streamer = TextIteratorStreamer(tokenizer,\n",
    "                                    timeout=10.,\n",
    "                                    skip_prompt=True,\n",
    "                                    skip_special_tokens=False)\n",
    "    generate_kwargs = dict(\n",
    "        input_ids=inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        # top_p=top_p,\n",
    "        # top_k=top_k,\n",
    "        temperature=0.8,\n",
    "        num_beams=1,\n",
    "    )\n",
    "    \n",
    "    t = Thread(target=ftmodel.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "    \n",
    "    # outputs = []\n",
    "    res = \"\"\n",
    "    for text in streamer:\n",
    "        if \"[\" in text or \"]\" in text:\n",
    "            break\n",
    "        res += text\n",
    "        print(text, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\nPlaylists: \", \", \".join(get_playlists(msg, 3)))\n",
    "    \n",
    "    prompt += res.strip()\n",
    "    # break\n",
    "print(\"---\")\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
